<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FZ43FV0KSZ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      
      gtag('config', 'G-FZ43FV0KSZ');
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Alejandro Lozano</title>
    <meta name="author" content="Alejandro Lozano">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="./images/m.png">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>
  <body>
    <!-- Triangle Mesh Background -->
    <canvas id="triangle-mesh" style="position: fixed; top: 0; left: 0; width: 100%; height: 100%; z-index: -1; pointer-events: none;"></canvas>
    
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px" class="profile-row">
                  <td style="padding:20px 2.5%;width:55%;vertical-align:middle" class="profile-text-cell">
                    <p class="name" style="text-align: center;">
                      Alejandro Lozano
                    </p>
                    <div class="profile-photo-mobile">
                      <a href="images/alejandro.jpg">
                        <img 
                          style="width:100%; max-width:100%; border-radius:50%;" 
                          alt="profile photo" 
                          src="images/alejandro.jpg" 
                          class="hoverZoomLink profile-photo" 
                          alt="Alejandro Lozano headshot">
                      </a>
                    </div>
                    <p>I am a PhD candidate at the <a href="https://ai.stanford.edu/">Stanford Artificial Intelligence Laboratory (SAIL)</a>  working on vision-language foundation models. I'm fortunate to be advised by <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a> and to be supported by the <a href="https://arcinstitute.org/">Arc Institute</a>. I am deeply grateful to NVIDIA, Amazon, and HAI for generously funding  my research. I also work part time at Microsoft Research under the supervision of <a href="https://www.microsoft.com/en-us/research/people/hoifung-poon/">Hoifung Poon</a> (hosted by Jeya Maria Jose
                      ) and I am an editor of multimodal AI for scientfic resarch chapter for the  <a href="   
                      https://hai.stanford.edu/ai-index">AI Index</a>.
                   
                    </p>  
                    <p>
                      My  work focuses on multimodal learning, multimodal retrieval-augmentation, agent-based systems, and the intersection of those topics with real-world applications, with an emphasis on precision medicine. During my free time, I like to hike around the bay area, play guitar, and meditate. 
                    </p>

                 
                    <p class="social-links" style="text-align:center">
                      <!-- <a href="CV">CV</a> &nbsp;/&nbsp; -->
                      <a href="https://scholar.google.com/citations?user=EBPnTFMAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                      <a href="https://github.com/alejandro-lozano-dev">Github</a> &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/ale9806/">LinkedIn</a>  &nbsp;/&nbsp;
                      <a href="https://x.com/Ale9806_">Twitter</a> &nbsp;/&nbsp;
                    </p>
                  </td>
                  <td style="padding:20px 2.5%;width:30%;max-width:30%" class="profile-photo-cell">
                    <a href="images/alejandro.jpg">
                      <img 
                        style="width:100%; max-width:100%; border-radius:50%;" 
                        alt="profile photo" 
                        src="images/alejandro.jpg" 
                        class="hoverZoomLink profile-photo" 
                        alt="Alejandro Lozano headshot">
                    </a>
                  </td>
                </tr>
              
              </tbody>
              
            </table>


            <!-- RECENT NEWS -->
            <section id="news" class="news-section">
              <h2>Recent News</h2>
              <ul class="news-list">
                <li class="news-item">
                  <span class="news-date">[Jan 2026]</span>
                  <span class="news-content">1 paper accepted to ICLR 2026, see you in Brazil.</span>
                </li>
                <li class="news-item">
                  <span class="news-date">[Dec 2025]</span>
                  <span class="news-content">1 paper accepted to EURIPS 2025, see you in Copenhagen.</span>
                </li>
                <li class="news-item">
                  <span class="news-date">[June 2025]</span>
                  <span class="news-content">I am interning at Microsoft Research.</span>
                </li>
                <li class="news-item">
                  <span class="news-date">[March 2025]</span>
                  <span class="news-content">Awarded Nvidia grant.</span>
                </li>
                <li class="news-item">
                  <span class="news-date">[February 2025]</span>
                  <span class="news-content">3 papers accepted to CVPR 2025.</span>
                </li>
                <li class="news-item">
                  <span class="news-date">[January 2025]</span>
                  <span class="news-content">2 papers accepted to ICLR 2025</span>
                </li>
                <li class="news-item">
                  <span class="news-date">[December 2024]</span>
                  <span class="news-content">1 paper accepted to NEJM AI.</span>
                </li>
                <li class="news-item">
                  <span class="news-date">[September 2024]</span>
                  <span class="news-content">1 paper accepted to NeurIPS 2024.</span>
                </li>
              </ul>
            </section>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              

            <!-- Selected Publications -->
              <tbody>
                <tr>
                  <td style="padding:20px 2.5%;width:100%;vertical-align:middle"> 
                    <h2>Selected Publications</h2>
                    <em> (*) denotes co-first authorship</em>. For a full list of publications, please check my <a href="https://scholar.google.com/citations?user=EBPnTFMAAAAJ&hl=en">Google Scholar</a> 
                    <!-- <p>
                      Preamble
                      </p> -->
                  </td>
                </tr>
              </tbody>
            </table>
            <!-- Carousel Container for Mobile -->
            <div class="papers-carousel-container">
              <div class="carousel-nav carousel-prev" onclick="changePaper(-1)">‚Äπ</div>
              <div class="papers-carousel-wrapper">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" class="papers-table">
              <tbody>

              

                <!-- Paper: Biomedica  -->
                <tr bgcolor="#ffffff" class="paper-item">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig-biomedica.jpeg' width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://minwoosun.github.io/biomedica-website/">
                    <span class="papertitle">BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature</span>
                    </a>
                    <br>
                    <strong>Alejandro Lozano</strong> *,
                    Min Woo Sun*,
                    James Burgess*,
                    Liangyu Chen,
                    Jeffrey J. Nirschl,
                    Jeffrey Gu,
                    Ivan Lopez,
                    Josiah Aklilu,
                    Anita Rau,
                    Austin Wolfgana Katzer,
                    Collin Chiu,
                    Xiaohan Wang,
                    Alfred Seunghoon Song,
                    Robert Tibshirani,
                    Serena Yeung-Levy
                    <br>
                    <em>CVPR 2025</em>
                    <br>
                        <a href="https://minwoosun.github.io/biomedica-website/">project page</a> /
                        <a href="https://arxiv.org/abs/2505.22787">paper</a>  / 
                        <a href="https://github.com/minwoosun/biomedica-etl">code</a> / 
                        <a href="https://huggingface.co/BIOMEDICA">data</a> 
                    <br>
               
                    <p></p>
                    We introduce BIOMEDICA, a framework to transform PMC-OA into a comprehensive dataset of over 24 million image-text pairs with expert-guided annotations, enabling the training of state-of-the-art  biomedical vision-language models across diverse tasks and domains.
                  </td>
                </tr>

                    <!-- Paper: SR -->
                   <tr bgcolor="#ffffff" class="paper-item">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                     <div class="one" >
                          <img src='images/med_evidence.png' width="200" style="padding-top: 30px;">
                    </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2505.22787">
                      <span class="papertitle">Can Large Language Models Match the Conclusions of Systematic Reviews?</span>
                      </a>
                      <br>
                      Christopher Polzak*, 
                      <strong> Alejandro Lozano </strong>*, 
                      Min Woo Sun*, 
                      James Burgess, 
                      Yuhui Zhang, 
                      Kevin Wu, 
                      Serena Yeung-Levy
                      <br>
                      <em>TBD 2025</em>
                      <br>
                        <a href="https://zy-f.github.io/website-med-evidence/">project page</a> /
                    <a href="https://arxiv.org/abs/2501.07171">paper</a> / 
                    <a href="https://github.com/zy-f/med-evidence">code</a> / 
                    <a href="https://huggingface.co/datasets/clcp/med-evidence">data</a> 

                   
                     
                      <br
                      <p></p>
                     Can LLMs match the conclusions of systematic reviews written by clinical experts when given access to the same studies? To explore this question, we present MedEvidence, A human-curated benchmark of 284 questions (from 100 open-access SRs) across 10 medical specialties. 
                    </td>
                  </tr>


                <!-- Paper: MicroVQA  -->
                <tr bgcolor="#ffffff" class="paper-item">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_microvqa.png'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="assets/microvqa.pdf">
                    <span class="papertitle">MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</span>
                    </a>
                    <br>
                    James Burgess*,
                    Jeffrey J Nirschl*,
                    Laura Bravo-S√°nchez*,
                    <strong> Alejandro Lozano</strong>,
                    Sanket Rajan Gupte,
                    Jesus G. Galaz-Montoya,
                    Yuhui Zhang,
                    Yuchang Su,
                    Disha Bhowmik,
                    Zachary Coman,
                    Sarina M. Hasan,
                    Alexandra Johannesson,
                    William D. Leineweber,
                    Malvika G Nair,
                    Ridhi Yarlagadda,
                    Connor Zuraski,
                    Wah Chiu,
                    Sarah Cohen,
                    Jan N. Hansen,
                    Manuel D Leonetti,
                    Chad Liu,
                    Emma Lundberg,
                    <br>
                    <em>CVPR 2025</em>
                    <br>
                    <a href="assets/microvqa.pdf">paper</a> / 
                    <a href="https://huggingface.co/datasets/microvqa/microvqa">data</a> 
                    <br>
                    <p></p>
                    MicroVQA is an expert-curated benchmark for research-level reasoning in biological microscopy. We also propose a method for making multiple-choice VQA more challenging.
                  </td>
                </tr>
              
              
               

             


                   <!-- Paper: TTE -->
                   <tr bgcolor="#ffffff" class="paper-item">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                      <div class="one">
                        <img src='images/tte.png'   width="200" style="padding-top: 25px;">
                      </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/pdf/2411.09361">
                      <span class="papertitle">Time-to-Event Pretraining for 3D Medical Imaging</span>
                      </a>
                      <br>
                      Zepeng Huo*,, 
                      Jason Alan Fries*,
                      <strong> Alejandro Lozano </strong>*,
                      Jeya Maria Jose Valanarasu, 
                      Ethan Steinberg, 
                      Louis Blankemeier, 
                      Akshay S. Chaudhari, 
                      Curtis Langlotz, 
                      Nigam H. Shah
                      <br>
                      <em>ICLR 2025</em>
                      <br>
                      <a href="https://arxiv.org/pdf/2411.09361">paper</a> 
                      <br
                      <p></p>
                      We propose the first time-to-event pretraining framework for 3D medical imaging models that leverages large-scale temporal supervision from paired longitudinal electronic health records.
                    </td>
                  </tr>



                      <!-- Paper: Video Action Differencing  -->
                      <tr bgcolor="#ffffff" class="paper-item">
                        <td style="padding:20px;width:25%;vertical-align:middle">
                          <div class="one">
                            <img src='images/fig1_left-viddiff.jpg'   width="200">
                          </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                          <a href="https://arxiv.org/abs/2503.07860">
                          <span class="papertitle">Video Action Differencing</span>
                          </a>
                          <br>
                         James Burgess,
                          Xiaohan Wang,
                          Yuhui Zhang,
                          Anita Rau,
                          <strong> Alejandro Lozano</strong>,
                          Lisa Dunlap,
                          Trevor Darrell,
                          Serena Yeung-Levy
                          <br>
                          <em>ICLR 2025</em>
                          <br>
                          <a href="https://arxiv.org/abs/2503.07860">paper</a> / 
                          <a href="https://huggingface.co/datasets/viddiff/VidDiffBench">data</a> 
                          <p></p>
                          We propose Video Action Differencing (VidDiff), a new task aimed at detecting subtle differences in how actions are performed across pairs of videos. To support this task, we introduce a benchmark spanning a diverse set of skilled actions, along with a baseline  and agentic workflow to investigate the limitations of current VLMs.
                        </td>
                      </tr>
      
      
      

        
                <!-- Paper: MicroBench -->
                <tr bgcolor="#ffffff" class="paper-item">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_ubench.jpg'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://ale9806.github.io/uBench-website/">
                    <span class="papertitle">Micro-Bench: A Vision-Language Benchmark for Microscopy Understanding</span>
                    </a>
                    <br>
                    <strong>Alejandro Lozano</strong> </a>*,
                    Jeffrey Nirschl</a>*,
                   James Burgess,
                    Sanket Rajan Gupte</a>,
                    Yuhui Zhang</a>,
                    Alyssa Unell</a>,
                    Serena Yeung-Levy</a>
                    <br>
                    <em>NeurIPS  2024</em>
                    <br>
                    <a href="https://ale9806.github.io/uBench-website/">project page</a> / 
                    <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/36b31e1bb8ecd4f4081686448e9eff2d-Abstract-Datasets_and_Benchmarks_Track.html">paper</a> /
                    <a href="https://github.com/yeung-lab/u-Bench">code</a> /
                    <a href="https://huggingface.co/datasets/jnirschl/uBench">data</a> 
                    <br>
              
                    <p></p>
                    <p>
                       A Vision-Language Benchmark for Microscopy Understanding, featuring 17,000 microscopy images sourced from 24 publicly available datasets. As the most diverse microscopy benchmark to date, it spans light (LM), fluorescence (FM), and electron microscopy (EM) across 8 sub-modalities, 91 distinct cell, tissue, and structure types, and 24 staining techniques. Microbench supports tasks including closed-form visual question answering (VQA), object detection, and segmentation.
                  </td>
                </tr>



                       <!-- Paper: Medalign -->
                <tr bgcolor="#ffffff" class="paper-item">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/medalign.png'   width="200" style="padding-top: -20px;">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://som-shahlab.github.io/medalign-website/">
                    <span class="papertitle">Medalign: A clinician-generated dataset for instruction following with electronic medical records</span>
                    </a>
                    <br>
                    Scott L Fleming*, <strong>Alejandro Lozano</strong>*, William J Haberkorn*, Jenelle A Jindal*, Eduardo Reis*, Rahul Thapa, Louis Blankemeier, Julian Z Genkins, Ethan Steinberg, Ashwin Nayak, Birju Patel, Chia-Chun Chiang, Alison Callahan, Zepeng Huo, Sergios Gatidis, Scott Adams, Oluseyi Fayanju, Shreya J Shah, Thomas Savage, Ethan Goh, Akshay S Chaudhari, Nima Aghaeepour, Christopher Sharp, Michael A Pfeffer, Percy Liang, Jonathan H Chen, Keith E Morse, Emma P Brunskill, Jason A Fries, Nigam H Shah
                    <br>
                    <em>AAAI 2024 / ML4H 2023 (Best Paper Award)</em>
                    <br>
                    <a href="https://som-shahlab.github.io/medalign-website/">project page</a> / 
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/30205">paper</a> /
                    <a href="https://github.com/som-shahlab/medalign">code</a>  /
                    <a href="https://redivis.com/ShahLab/datasets">data</a>
                    <br>
                    <p></p>
                    <p>
                    We introduce MedAlign, a benchmark of 983 natural language instructions about EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes human-written reference responses, and provides 276 full longitudinal EHRs for grounding instruction-response pairs. 
                    </p>
                  </td>
                </tr>
                
                
                <!-- Paper: CLINFO  -->
                <tr bgcolor="#ffffff" class="paper-item">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/clinfo.png'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://www.worldscientific.com/doi/abs/10.1142/9789811286421_0002">
                    <span class="papertitle">Clinfo. ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature</span>
                    </a>
                    <br>
                    <strong> Alejandro Lozano</strong>,
                    Scott L Fleming, 
                    Chia-Chun Chiang, 
                    Nigam Shah
                    <br>
                    <em>Pacific Symposium on Biocomputing  2024 (Oral)</em>
                    <br>
                    <a href="https://www.worldscientific.com/doi/abs/10.1142/9789811286421_0002">paper</a>
                    /
                    <a href="https://github.com/som-shahlab/Clinfo.AI">code</a>
                    <p></p>
                    <p>
                     Introducing Clinfo.ai, the first open-source agentic system designed to answer medical questions using scientific literature. Clinfo.ai employs a chain of large language models to convert a question into a query and explore the most relevant literature to provide an up-to-date answer.
                    </p>
                  </td>
                </tr>

                <!-- Paper: O2vae  -->
                <tr bgcolor="#ffffff" class="paper-item">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_o2vae.png'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://www.nature.com/articles/s41467-024-45362-4">
                    <span class="papertitle">Orientation-invariant autoencoders learn robust representations for shape profiling of cells and organelles</span>
                    </a>
                    <br>
                    James Burgess,
                    Jeffrey J. Nirschl,
                    Maria-Clara Zanellati,
                    <strong> Alejandro Lozano</strong>,
                    Sarah Cohen,
                    Serena Yeung-Levy
                    <br>
                    <em>Nature Communications 2024</em>
                    <br>
                    <a href="https://www.nature.com/articles/s41467-024-45362-4">paper</a> 
                    /
                    <a href="https://github.com/jmhb0/o2vae">code</a>
                    <p></p>
                    <p>
                   Unsupervised shape representations of cells and organelles are often erroneously sensitive to image orientation. We introduce O2VAE, an orientation-invariant autoencoder that mitigates this issue by using equivariant convolutional network encoders.


                    </p>
                  </td>
                </tr>


                


              <!-- Footer attribution for  the website template-->
              </tbody>
            </table>
              </div>
              <div class="carousel-nav carousel-next" onclick="changePaper(1)">‚Ä∫</div>
              <div class="carousel-indicators"></div>
            </div>
            <table class="teaching-section" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px 2.5%;width:100%;vertical-align:middle">
                    <h2>Teaching</h2>
                    <ul class="teaching-list">
                      <li class="teaching-item">
                        <div class="teaching-role">Stanford AI4ALL Medical AI Lead Mentor</div>
                        <div class="teaching-meta">Stanford, 2024 and 2025</div>
                      </li>
                      <li class="teaching-item">
                        <div class="teaching-role">Head Teaching assistant, CS 235: <em>Computational Methods for Biomedical Image Analysis and Interpretation</em></div>
                        <div class="teaching-meta">Stanford, 2025</div>
                      </li>
                      <li class="teaching-item">
                        <div class="teaching-role">Teaching assistant, CS 235: <em>Computational Methods for Biomedical Image Analysis and Interpretation</em></div>
                        <div class="teaching-meta">Stanford, 2022</div>
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:left;font-size:small;">
                      I stole this website template from <a href="https://jonbarron.info/">Jon Barron</a> who published his source code <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
            
            <!-- Visitor Map Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>Visitor Map</h2>
                    <p style="margin-bottom: 15px; color: #666;">
                      See where visitors are coming from around the world
                    </p>
                    <div id="visitor-map" style="width:100%; height:500px; border-radius:8px; overflow:hidden; box-shadow: 0 4px 12px rgba(0,0,0,0.1);"></div>
                    <p style="text-align:center; margin-top: 10px; font-size: 12px; color: #999;">
                      <span id="visitor-count">0</span> unique locations visited
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
    </table>
    
    <!-- Leaflet CSS -->
    <link rel="stylesheet" href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css" />
    <link rel="stylesheet" href="https://unpkg.com/leaflet.markercluster@1.5.3/dist/MarkerCluster.css" />
    <link rel="stylesheet" href="https://unpkg.com/leaflet.markercluster@1.5.3/dist/MarkerCluster.Default.css" />
    <!-- Leaflet JS -->
    <script src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js"></script>
    <script src="https://unpkg.com/leaflet.markercluster@1.5.3/dist/leaflet.markercluster.js"></script>
    
    <script>
      let currentPaperIndex = 0;
      const papers = document.querySelectorAll('.paper-item');
      const indicatorsContainer = document.querySelector('.carousel-indicators');
      
      // Make paper items clickable
      papers.forEach(paper => {
        paper.addEventListener('click', function(e) {
          // Don't navigate if clicking on a link directly
          if (e.target.tagName === 'A' || e.target.closest('a')) {
            return;
          }
          
          // Find the first link (title/project page link)
          const firstLink = paper.querySelector('a');
          if (firstLink && firstLink.href) {
            window.location.href = firstLink.href;
          }
        });
      });
      
      // Initialize carousel
      function initCarousel() {
        if (window.innerWidth <= 768) {
          // Mobile view - show carousel
          papers.forEach((paper, index) => {
            paper.classList.remove('active');
            if (index === 0) {
              paper.classList.add('active');
            }
          });
          
          // Create indicators
          if (indicatorsContainer) {
            indicatorsContainer.innerHTML = '';
            papers.forEach((paper, index) => {
              const indicator = document.createElement('button');
              indicator.className = 'carousel-indicator' + (index === 0 ? ' active' : '');
              indicator.setAttribute('aria-label', `Go to paper ${index + 1}`);
              indicator.onclick = () => goToPaper(index);
              indicatorsContainer.appendChild(indicator);
            });
          }
        } else {
          // Desktop view - show all papers
          papers.forEach(paper => {
            paper.classList.add('active');
          });
        }
      }
      
      function changePaper(direction) {
        if (window.innerWidth > 768) return; // Only work on mobile
        
        currentPaperIndex += direction;
        
        if (currentPaperIndex < 0) {
          currentPaperIndex = papers.length - 1;
        } else if (currentPaperIndex >= papers.length) {
          currentPaperIndex = 0;
        }
        
        goToPaper(currentPaperIndex);
      }
      
      function goToPaper(index) {
        if (window.innerWidth > 768) return; // Only work on mobile
        
        currentPaperIndex = index;
        
        papers.forEach((paper, i) => {
          paper.classList.remove('active');
          if (i === index) {
            paper.classList.add('active');
          }
        });
        
        // Update indicators
        const indicators = indicatorsContainer.querySelectorAll('.carousel-indicator');
        indicators.forEach((indicator, i) => {
          indicator.classList.remove('active');
          if (i === index) {
            indicator.classList.add('active');
          }
        });
      }
      
      // Initialize on load
      window.addEventListener('load', initCarousel);
      window.addEventListener('resize', initCarousel);
      
      // Touch swipe support
      let touchStartX = 0;
      let touchEndX = 0;
      
      const carouselWrapper = document.querySelector('.papers-carousel-wrapper');
      if (carouselWrapper) {
        carouselWrapper.addEventListener('touchstart', (e) => {
          touchStartX = e.changedTouches[0].screenX;
        }, { passive: true });
        
        carouselWrapper.addEventListener('touchend', (e) => {
          touchEndX = e.changedTouches[0].screenX;
          handleSwipe();
        }, { passive: true });
      }
      
      function handleSwipe() {
        if (window.innerWidth > 768) return; // Only work on mobile
        
        const swipeThreshold = 50;
        const diff = touchStartX - touchEndX;
        
        if (Math.abs(diff) > swipeThreshold) {
          if (diff > 0) {
            // Swipe left - next paper
            changePaper(1);
          } else {
            // Swipe right - previous paper
            changePaper(-1);
          }
        }
      }
    </script>
    
    <!-- Visitor Map Script -->
    <script>
      // Initialize the map
      function initVisitorMap() {
        // Create map centered on the world with globe view
        const map = L.map('visitor-map', {
          center: [20, 0],
          zoom: 2,
          minZoom: 2,
          maxZoom: 5,
          worldCopyJump: true
        });
        
        // Add a world map style (using CartoDB Positron for a cleaner globe look)
        L.tileLayer('https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png', {
          attribution: '¬© <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors, ¬© <a href="https://carto.com/attributions">CARTO</a>',
          maxZoom: 5,
          subdomains: 'abcd'
        }).addTo(map);
        
        // Get stored visitor locations
        let visitorLocations = JSON.parse(localStorage.getItem('visitorLocations') || '[]');
        
        // Get current visitor's location
        fetch('https://ipapi.co/json/')
          .then(response => response.json())
          .then(data => {
            if (data.city && data.country_name) {
              // Use city center coordinates instead of exact location
              // Add small random offset to city center for privacy (within ~5km)
              const cityLat = parseFloat(data.latitude) || 0;
              const cityLng = parseFloat(data.longitude) || 0;
              
              // Add small random offset (approximately 0.05 degrees = ~5km)
              const offsetLat = (Math.random() - 0.5) * 0.1;
              const offsetLng = (Math.random() - 0.5) * 0.1;
              
              const location = {
                lat: cityLat + offsetLat,
                lng: cityLng + offsetLng,
                city: data.city || 'Unknown',
                country: data.country_name || 'Unknown',
                timestamp: new Date().toISOString()
              };
              
              // Check if this city already exists (same city and country)
              const exists = visitorLocations.some(loc => 
                loc.city === location.city && loc.country === location.country
              );
              
              if (!exists) {
                visitorLocations.push(location);
                // Keep only last 500 locations to avoid localStorage limits
                if (visitorLocations.length > 500) {
                  visitorLocations = visitorLocations.slice(-500);
                }
                localStorage.setItem('visitorLocations', JSON.stringify(visitorLocations));
              }
            }
            
            // Plot all locations on the map
            plotLocations(map, visitorLocations);
          })
          .catch(error => {
            console.error('Error fetching location:', error);
            // Still plot existing locations even if current location fetch fails
            plotLocations(map, visitorLocations);
          });
      }
      
      function plotLocations(map, locations) {
        // Update visitor count (count unique cities)
        const uniqueCities = new Set(locations.map(loc => `${loc.city}, ${loc.country}`));
        document.getElementById('visitor-count').textContent = uniqueCities.size;
        
        // Create a marker cluster group for better performance
        const markers = L.markerClusterGroup({
          maxClusterRadius: 80,
          spiderfyOnMaxZoom: true,
          showCoverageOnHover: false,
          zoomToBoundsOnClick: true,
          disableClusteringAtZoom: 4
        });
        
        // Add markers for each location (city-level)
        locations.forEach(location => {
          const marker = L.marker([location.lat, location.lng], {
            title: `${location.city}, ${location.country}`
          });
          const popupText = `<b>${location.city}, ${location.country}</b><br>Visited: ${new Date(location.timestamp).toLocaleDateString()}`;
          marker.bindPopup(popupText);
          markers.addLayer(marker);
        });
        
        map.addLayer(markers);
        
        // Keep the map at world view (zoom level 2)
        map.setView([20, 0], 2);
      }
      
      // Initialize map when page loads
      window.addEventListener('load', () => {
        // Small delay to ensure map container is rendered
        setTimeout(initVisitorMap, 100);
      });
    </script>
    
    <!-- Triangle Mesh Script -->
    <script>
      (function() {
        const canvas = document.getElementById('triangle-mesh');
        const ctx = canvas.getContext('2d');
        let scrollY = 0;
        let time = 0;
        let needsRedraw = true;
        
        // Set canvas size
        function resizeCanvas() {
          canvas.width = window.innerWidth;
          canvas.height = window.innerHeight;
          needsRedraw = true;
        }
        resizeCanvas();
        window.addEventListener('resize', resizeCanvas);
        
        // Track scroll (but don't use it for movement - only for color effects if needed)
        window.addEventListener('scroll', () => {
          scrollY = window.pageYOffset || window.scrollY || 0;
        }, { passive: true });
        
        // Triangle mesh parameters - larger and more visible
        const triangleSize = 60;
        const spacing = 80; // More spacing between triangles
        
        // Generate triangle points
        function createTriangle(x, y, size, angle = 0) {
          const h = size * 0.866; // height of equilateral triangle
          const cos = Math.cos(angle);
          const sin = Math.sin(angle);
          
          // Center point
          const cx = x + size / 2;
          const cy = y - h / 3;
          
          // Three vertices
          const v1 = { x: x - cx, y: y - cy };
          const v2 = { x: x + size - cx, y: y - cy };
          const v3 = { x: x + size / 2 - cx, y: y - h - cy };
          
          // Rotate around center
          const rotate = (v) => ({
            x: v.x * cos - v.y * sin + cx,
            y: v.x * sin + v.y * cos + cy
          });
          
          return [rotate(v1), rotate(v2), rotate(v3)];
        }
        
        // Draw triangle with gradient
        function drawTriangle(triangle, color, strokeColor) {
          ctx.beginPath();
          ctx.moveTo(triangle[0].x, triangle[0].y);
          ctx.lineTo(triangle[1].x, triangle[1].y);
          ctx.lineTo(triangle[2].x, triangle[2].y);
          ctx.closePath();
          
          // Fill with gradient
          const gradient = ctx.createLinearGradient(
            triangle[0].x, triangle[0].y,
            triangle[2].x, triangle[2].y
          );
          gradient.addColorStop(0, color);
          gradient.addColorStop(1, color.replace(/[\d\.]+\)$/g, '0.3)'));
          
          ctx.fillStyle = gradient;
          ctx.fill();
          
          // Stroke
          ctx.strokeStyle = strokeColor;
          ctx.lineWidth = 1.5;
          ctx.stroke();
        }
        
        // Draw function - very slow animation
        function draw() {
          // Clear canvas
          ctx.clearRect(0, 0, canvas.width, canvas.height);
          
          // Very slow automatic movement
          time += 0.05; // Very slow increment
          
          // Calculate offsets based only on very slow time movement (no scroll effect)
          const triangleHeight = triangleSize * 0.866; // height of equilateral triangle
          const rowHeight = triangleHeight + spacing;
          const slowTimeOffset = time * 0.5; // Very slow time-based movement
          const offsetY = slowTimeOffset % rowHeight;
          const offsetX = Math.sin(time * 0.01) * 2; // Very subtle horizontal drift
          
          // No rotation
          const rotation = 0;
          
          // Create proper tessellation - triangles that fit together without overlap
          const cols = Math.ceil(canvas.width / (triangleSize + spacing)) + 2;
          const rows = Math.ceil(canvas.height / rowHeight) + 2;
          
          for (let row = -1; row < rows; row++) {
            for (let col = -1; col < cols; col++) {
              const isUpsideDown = row % 2 === 1;
              
              // Offset odd rows by half triangle width to create tessellation, with spacing
              const x = col * (triangleSize + spacing) + (isUpsideDown ? (triangleSize + spacing) / 2 : 0) + offsetX;
              const y = row * rowHeight - offsetY;
              
              // Create triangle based on orientation
              let triangle;
              if (isUpsideDown) {
                // Downward pointing triangle
                const topY = y;
                triangle = [
                  { x: x, y: topY },
                  { x: x + triangleSize, y: topY },
                  { x: x + triangleSize / 2, y: topY + triangleHeight }
                ];
              } else {
                // Upward pointing triangle
                const bottomY = y + triangleHeight;
                triangle = [
                  { x: x, y: bottomY },
                  { x: x + triangleSize, y: bottomY },
                  { x: x + triangleSize / 2, y: bottomY - triangleHeight }
                ];
              }
              
              // Apply rotation only if needed (subtle)
              if (Math.abs(rotation) > 0.001) {
                const cx = x + triangleSize / 2;
                const cy = y + triangleHeight / 2;
                const cos = Math.cos(rotation);
                const sin = Math.sin(rotation);
                
                triangle = triangle.map(v => ({
                  x: (v.x - cx) * cos - (v.y - cy) * sin + cx,
                  y: (v.x - cx) * sin + (v.y - cy) * cos + cy
                }));
              }
              
              // Dynamic color based on scroll and position only
              const scrollIntensity = (Math.sin(scrollY * 0.003) + 1) * 0.5;
              const wave1 = Math.sin(x * 0.02);
              const wave2 = Math.cos(y * 0.02);
              const positionEffect = (wave1 + wave2) * 0.5;
              
              // More noticeable colors with blue tint
              const baseR = 200 + scrollIntensity * 30;
              const baseG = 210 + scrollIntensity * 25;
              const baseB = 240 + scrollIntensity * 15;
              
              const r = Math.floor(baseR + positionEffect * 20);
              const g = Math.floor(baseG + positionEffect * 15);
              const b = Math.floor(baseB + positionEffect * 10);
              
              // 99.375% transparent (0.625% opacity - 50% more transparent)
              const alpha = 0.00625;
              const color = `rgba(${r}, ${g}, ${b}, ${alpha})`;
              const strokeColor = `rgba(26, 115, 232, ${alpha * 1.5})`;
              
              drawTriangle(triangle, color, strokeColor);
            }
          }
        }
        
        // Continuous slow animation
        function animate() {
          draw();
          requestAnimationFrame(animate);
        }
        
        // Start animation
        animate();
      })();
    </script>
  </body>
</html>
