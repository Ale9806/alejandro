<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FZ43FV0KSZ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      
      gtag('config', 'G-FZ43FV0KSZ');
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Alejandro Lozano</title>
    <meta name="author" content="Alejandro Lozano">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="./images/m.png">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>
  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:55%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Alejandro Lozano
                    </p>
            
                    <p>I am a PhD candidate at the Stanford Artificial Intelligence Laboratory (SAIL) working on vision-langue foundation models. I'm fortunate to be advised by <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a> and to be supported by the <a href="https://arcinstitute.org/">Arc Institute</a>. 
                    </p>
                    <p>
                      My  work focuses on multimodal learning, multimodal retrieval-augmentation, agent-based systems and the intersection of those topics with real-world applications, with an emphasis on precision medicine. During my free time, I like to ... I don't have free time ;/ 
                    </p>

                 
                    <p style="text-align:center">
                      <!-- <a href="CV">CV</a> &nbsp;/&nbsp; -->
                      <a href="https://scholar.google.com/citations?user=EBPnTFMAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                      <a href="https://x.com/Ale9806_">Twitter</a> &nbsp;/&nbsp;
                      <a href="https://github.com/Ale9806/">Github</a> &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/ale9806/">LinkedIn</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:30%;max-width:30%">
                    <a href="images/alejandro.jpg">
                      <img 
                        style="width:100%; max-width:100%; border-radius:50%;" 
                        alt="profile photo" 
                        src="images/alejandro.jpg" 
                        class="hoverZoomLink" 
                        alt="Alejandro Lozano headshot">
                    </a>
                  </td>
                </tr>
              
              </tbody>
              
            </table>

            <section id="news" style="padding:20px;width:100%;vertical-align:middle;;margin-top:-70px;">
              <h2>Recent News:</h2>
              <table style="width: 40%; border-collapse: collapse;">
                <style>
                  @media (max-width: 1000px) {
                    table {
                      width: 100%;
                    }
                  }
                </style>
           
                <tr>
                  <td style="padding:1px;">[March 2025]</td>
                  <td style="padding:1px;">Awarded Nvidia grant.</td>
                </tr>
                <tr>
                  <td style="padding:1px;">[February 2025]</td>
                  <td style="padding:1px;">3 papers accepted to CVPR 2025.</td>
                </tr>
                <tr>
                  <td style="padding:1px;">[January 2025]</td>
                  <td style="padding:1px;">2 papers accepted to ICLR 2025</td>
                </tr>
                <tr>
                  <td style="padding:1px;">[December 2024]</td>
                  <td style="padding:1px;">1 paper accepted to NEJM AI.</td>
                </tr>
                <tr>
                  <td style="padding:1px;">[September 2024]</td>
                  <td style="padding:1px;">1 paper accepted to NeurIPS 2024.</td>
                </tr>
              </table>
            </section>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            

              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">


               
                    <h2>Selected Publications</h2>
                    <!-- <p>
                      Preamble
                      </p> -->
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

              

                <!-- Paper: Biomedica  -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig-biomedica.jpeg' width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://minwoosun.github.io/biomedica-website/">
                    <span class="papertitle">BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature</span>
                    </a>
                    <br>
                    <strong>Alejandro Lozano</strong> *,
                    Min Woo Sun*,
                    James Burgess*,
                    Liangyu Chen,
                    Jeffrey J. Nirschl,
                    Jeffrey Gu,
                    Ivan Lopez,
                    Josiah Aklilu,
                    Anita Rau,
                    Austin Wolfgana Katzer,
                    Collin Chiu,
                    Xiaohan Wang,
                    Alfred Seunghoon Song,
                    Robert Tibshirani,
                    Serena Yeung-Levy
                    <br>
                    <em>Preprint</em>
                    <br>
                    <a href="https://minwoosun.github.io/biomedica-website/">project page</a> /
                    <a href="https://arxiv.org/abs/2501.07171">arxiv</a> / 
                    <a href="https://github.com/minwoosun/biomedica-etl">code</a> / 
                    <a href="https://huggingface.co/BIOMEDICA">data</a> 
                    <br>
                    <em>*co-first authorship</em>
                    <p></p>
                    We introduce BIOMEDICA, an open-source framework that transforms the PubMed Central Open Access subset into a comprehensive dataset of over 24 million image-text pairs with expert-guided annotations, enabling state-of-the-art performance in biomedical vision-language models across diverse tasks and domains.
                  </td>
                </tr>


              

             


                   <!-- Paper: TTE -->
                   <tr bgcolor="#ffffff">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                      <div class="one">
                        <img src='images/tte.png'   width="200">
                      </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/pdf/2411.09361">
                      <span class="papertitle">Time-to-Event Pretraining for 3D Medical Imaging</span>
                      </a>
                      <br>
                      Zepeng Huo*,, 
                      Jason Alan Fries*,, 
                      <strong> Alejandro Lozano </strong>*,
                      Jeya Maria Jose Valanarasu, 
                      Ethan Steinberg, 
                      Louis Blankemeier, 
                      Akshay S. Chaudhari, 
                      Curtis Langlotz, 
                      Nigam H. Shah
                      <br>
                      <em>ICLR 2025</em>
                      <br>
                      <a href="https://arxiv.org/pdf/2411.09361">paper</a> 
                      <br
                      <em>*co-first authorship</em>
                      <p></p>
                      Using a dataset of 4.2 million 2D images and time-to-event distributions across thousands of EHR-derived tasks, we propose a new time-to-event pretraining framework for 3D medical imaging models that leverages large-scale temporal supervision from paired, longitudinal electronic health records.
                    </td>
                  </tr>



                      <!-- Paper: Video Action Differencing  -->
                      <tr bgcolor="#ffffff">
                        <td style="padding:20px;width:25%;vertical-align:middle">
                          <div class="one">
                            <img src='images/fig1_left-viddiff.jpg'   width="200">
                          </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                          <a href="assets/video-action-differencing.pdf">
                          <span class="papertitle">Video Action Differencing</span>
                          </a>
                          <br>
                         James Burgess,
                          Xiaohan Wang,
                          Yuhui Zhang,
                          Anita Rau,
                          <strong> Alejandro Lozano</strong>,
                          Lisa Dunlap,
                          Trevor Darrell,
                          Serena Yeung-Levy
                          <br>
                          <em>ICLR 2025</em>
                          <br>
                          <a href="assets/video-action-differencing.pdf">pdf</a> / 
                          <a href="https://huggingface.co/datasets/viddiff/VidDiffBench">benchmark</a> 
                          <p></p>
                          We propose video action differencing, a new task for detecting subtle variations in how actions are performed between two videos. We release a benchmark spaning diverse skilled actions, and a baseline agentic workflow.
                        </td>
                      </tr>
      
      
      

        
                <!-- Paper: MicroBench -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_ubench.jpg'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://ale9806.github.io/uBench-website/">
                    <span class="papertitle">Micro-Bench: A Vision-Language Benchmark for Microscopy Understanding</span>
                    </a>
                    <br>
                    <strong>Alejandro Lozano</strong> </a>*,
                    Jeffrey Nirschl</a>*,
                   James Burgess,
                    Sanket Rajan Gupte</a>,
                    Yuhui Zhang</a>,
                    Alyssa Unell</a>,
                    Serena Yeung-Levy</a>
                    <br>
                    <em>NeurIPS Datasets & Benchmarks 2024</em>
                    <br>
                    <a href="https://ale9806.github.io/uBench-website/">project page</a> / 
                    <a href="">arXiv</a> /
                    <a href="https://github.com/yeung-lab/u-Bench">code</a> 
                    <br>
                    <em>*co-first authorship</em>
                    <p></p>
                    <p>
                      A Vision-Language Benchmark for Microscopy Understanding.
                    </p>
                  </td>
                </tr>


                     <!-- Paper: MicroVQA  -->
                     <tr bgcolor="#ffffff">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <img src='images/fig_microvqa.png'   width="200">
                        </div>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="assets/microvqa.pdf">
                        <span class="papertitle">MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</span>
                        </a>
                        <br>
                        James Burgess*,
                        Jeffrey J Nirschl*,
                        Laura Bravo-S√°nchez*,
                        <strong> Alejandro Lozano</strong>,
                        Sanket Rajan Gupte,
                        Jesus G. Galaz-Montoya,
                        Yuhui Zhang,
                        Yuchang Su,
                        Disha Bhowmik,
                        Zachary Coman,
                        Sarina M. Hasan,
                        Alexandra Johannesson,
                        William D. Leineweber,
                        Malvika G Nair,
                        Ridhi Yarlagadda,
                        Connor Zuraski,
                        Wah Chiu,
                        Sarah Cohen,
                        Jan N. Hansen,
                        Manuel D Leonetti,
                        Chad Liu,
                        Emma Lundberg,
                        <br>
                        <em>Preprint</em>
                        <br>
                        <a href="assets/microvqa.pdf">pdf</a> / 
                        <a href="https://huggingface.co/datasets/microvqa/microvqa">benchmark</a> 
                        <br>
                        <em>*co-first authorship</em>
                        <p></p>
                        MicroVQA is an expert-curated benchmark for research-level reasoning in biological microscopy. We also propose a method for making multiple-choice VQA more challenging.
                      </td>
                    </tr>



                       <!-- Paper: MicroBench -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/medalign.png'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://som-shahlab.github.io/medalign-website/">
                    <span class="papertitle">Medalign: A clinician-generated dataset for instruction following with electronic medical records</span>
                    </a>
                    <br>
                    Scott L Fleming*, <strong>Alejandro Lozano</strong>*, William J Haberkorn*, Jenelle A Jindal*, Eduardo Reis*, Rahul Thapa, Louis Blankemeier, Julian Z Genkins, Ethan Steinberg, Ashwin Nayak, Birju Patel, Chia-Chun Chiang, Alison Callahan, Zepeng Huo, Sergios Gatidis, Scott Adams, Oluseyi Fayanju, Shreya J Shah, Thomas Savage, Ethan Goh, Akshay S Chaudhari, Nima Aghaeepour, Christopher Sharp, Michael A Pfeffer, Percy Liang, Jonathan H Chen, Keith E Morse, Emma P Brunskill, Jason A Fries, Nigam H Shah
                    <br>
                    <em>AAAI 2024 / ML4H 2023 (Best Paper Award)</em>
                    <br>
                    <a href="https://som-shahlab.github.io/medalign-website/">project page</a> / 
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/30205">paper</a> /
                    <a href="https://github.com/som-shahlab/medalign">code</a> 
                    <br>
                    <em>*co-first authorship</em>
                    <p></p>
                    <p>
                     Existing question answering datasets for electronic health record (EHR) data fail to capture the complexity of information needs and documentation burdens experienced by clinicians. To address these challenges, we introduce MedAlign, a benchmark dataset of 983 natural language instructions for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes clinician-written reference responses for 303 instructions, and provides 276 longitudinal EHRs for grounding instruction-response pairs. 
                    </p>
                  </td>
                </tr>


                <!-- Paper: O2vae  -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fig_o2vae.png'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://www.nature.com/articles/s41467-024-45362-4">
                    <span class="papertitle">Orientation-invariant autoencoders learn robust representations for shape profiling of cells and organelles</span>
                    </a>
                    <br>
                    James Burgess,
                    Jeffrey J. Nirschl,
                    Maria-Clara Zanellati,
                    <strong> Alejandro Lozano</strong>,
                    Sarah Cohen,
                    Serena Yeung-Levy
                    <br>
                    <em>Nature Communications 2024</em>
                    <br>
                    <a href="https://www.nature.com/articles/s41467-024-45362-4">paper</a> 
                    /
                    <a href="https://github.com/jmhb0/o2vae">code</a>
                    <p></p>
                    <p>
                      Unsupervised shape representations of cells and organelles are erroneously sensitive to image orientation, which we mitigate with equivariant convolutional network encoders in our method, O2VAE.
                    </p>
                  </td>
                </tr>

                <!-- Paper: Global Organelle Profiling  -->
                <tr bgcolor="#ffffff">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/clinfo.png'   width="200">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://www.worldscientific.com/doi/abs/10.1142/9789811286421_0002">
                    <span class="papertitle">Clinfo. ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature</span>
                    </a>
                    <br>
                    <strong> Alejandro Lozano</strong>,
                    Scott L Fleming, 
                    Chia-Chun Chiang, 
                    Nigam Shah
                    <br>
                    <em>Pacific Symposium on Biocomputing  2024 (Oral)</em>
                    <br>
                    <a href="https://www.worldscientific.com/doi/abs/10.1142/9789811286421_0002">paper</a>
                    /
                    <a href="https://github.com/som-shahlab/Clinfo.AI">code</a>
                    <p></p>
                    <p>
                      A retrieval-augmented chaing of large language models to answer medical questions using scientific literature. 
                    </p>
                  </td>
                </tr>
                


              <!-- Footer attribution for  the website template-->
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>Teaching</h2>
                    <br>
              

                    Teaching assistant, CS 235, <em>Computational Methods for Biomedical Image Analysis and Interpretation</em>, Stanford 2022.
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:left;font-size:small;">
                      I stole this website template from <a href="https://jonbarron.info/">Jon Barron</a> who published his source code <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
    </table>
  </body>
</html>
